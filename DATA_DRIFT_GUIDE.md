# Data Drift Detection & Simulation Guide

## ğŸ“Š Vue d'ensemble

Le **Data Drift** est un problÃ¨me critique en MLOps. Quand les donnÃ©es en production changent de distribution, les performances du modÃ¨le dÃ©gradent. Ce projet implÃ©mente une dÃ©tection **multi-mÃ©thodes** et des simulations de drift.

---

## ğŸ” Modules crÃ©Ã©s

### 1. **detect_drift.py** - DÃ©tecteur de Drift

Classe `DataDriftDetector` avec 3 mÃ©thodes statistiques :

#### **Kolmogorov-Smirnov (KS) Test**
```
Usage: DÃ©tecte diffÃ©rences dans les distributions
SensibilitÃ©: Queues de distribution
Seuil: p-value < 0.05
Cas: Excellente base de comparaison
```

#### **Population Stability Index (PSI)**
```
Formula: Î£ (curr% - ref%) * ln(curr% / ref%)
InterprÃ©tation:
  PSI < 0.05      â†’ Pas de drift
  PSI 0.05-0.1    â†’ Drift faible
  PSI 0.1-0.25    â†’ Drift modÃ©rÃ©
  PSI > 0.25      â†’ Drift significatif
```

#### **Welch t-test**
```
Usage: Compare means de deux distributions
Avantage: Pas d'assomption d'Ã©galitÃ© des variances
Robustesse: Bon pour donnÃ©es rÃ©elles
```

#### Exemple :
```python
from detect_drift import DataDriftDetector

# Charger donnÃ©es de rÃ©fÃ©rence (entraÃ®nement)
reference_data = pd.read_csv('training/data_processed.csv')

# Charger donnÃ©es actuelles (production)
current_data = pd.read_csv('production_data.csv')

# CrÃ©er dÃ©tecteur
detector = DataDriftDetector(reference_data)

# DÃ©tecter drift avec toutes les mÃ©thodes
results = detector.detect_drift(
    current_data,
    methods=['ks', 'psi', 'ttest'],
    ks_threshold=0.05,
    psi_threshold=0.1,
    ttest_threshold=0.05
)

# Afficher rapport
detector.print_report()

# Sauvegarder rÃ©sultats
detector.save_results('drift_results.json')
```

---

### 2. **simulate_drift.py** - Simulateur de Drift

Classe `DriftSimulator` pour gÃ©nÃ©rer diffÃ©rents types de drift :

#### **Mean Shift**
```
Scenario: Prix commencent Ã  monter/descendre systÃ©matiquement
Impact: Changement de tendance du marchÃ©
```

#### **Variance Shift**
```
Scenario: MarchÃ© devient plus volatil
Impact: Incertitude accrue, oscillations plus larges
```

#### **Outlier Injection**
```
Scenario: Ã‰vÃ©nements extrÃªmes (crash, pump)
Impact: 5-10% d'outliers injectÃ©s
```

#### **Covariate Shift**
```
Scenario: CorrÃ©lations entre features changent
Impact: Distributions marginales changent mais pas conditionnelles
```

#### **Concept Drift**
```
Scenario: Relation features-target change
Impact: Transformation non-linÃ©aire appliquÃ©e
```

#### **Gradual Drift**
```
Scenario: Changement lent mais continu
Impact: 5 batches avec shift progressif
```

#### **Seasonal Shift**
```
Scenario: Patterns de marchÃ© changent selon la saison
Impact: Composante sinusoÃ¯dale ajoutÃ©e
```

#### Exemple :
```python
from simulate_drift import DriftSimulator, generate_drift_scenarios

# Charger donnÃ©es
data = pd.read_csv('training/data_processed.csv')

# GÃ©nÃ©rer tous les scÃ©narios
scenarios = generate_drift_scenarios(data)

# AccÃ©der Ã  un scÃ©nario
mean_shift_data = scenarios['mean_shift']

# Utiliser pour tester le dÃ©tecteur
detector = DataDriftDetector(data)
results = detector.detect_drift(mean_shift_data)
```

---

### 3. **test_drift_detection.py** - Tests d'intÃ©gration

Script de test complet :

```bash
cd training
python test_drift_detection.py
```

**RÃ©sultats attendus** :

| ScÃ©nario | Drift Attendu | RÃ©sultat |
|----------|---------------|----------|
| baseline | Non | âœ“ PASS |
| mean_shift | Oui | âœ“ PASS |
| variance_shift | Oui | âœ“ PASS |
| outlier_injection | Oui | âœ“ PASS |
| concept_drift | Oui | âœ“ PASS |
| gradual_drift | Oui | âœ“ PASS |
| seasonal_shift | Oui | âœ“ PASS |

---

## ğŸ”„ IntÃ©gration CI/CD

### Workflow: `ml-training-pipeline.yml`

**JOB 1: check-drift**
```yaml
Steps:
  1. Checkout code
  2. Install dependencies (pandas, scipy)
  3. Detect data drift (run detect_drift.py)
  4. Parse results JSON
  5. Set retrain flag if drift detected
  6. Upload drift report as artifact
```

**DÃ©clencheur** :
- Tous les **dimanches Ã  02:00 UTC**
- **Manuellement** avec `force_retrain=true`

**Outputs** :
- `should_retrain`: true/false
- `drift_detection_report`: JSON report
- Historique: 30 jours de rapports

---

## ğŸ“ˆ Workflow de production

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Production Data   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Drift Detection    â”‚â”€â”€â”€â”€â”€â–º JSON Report
â”‚  (Weekly)           â”‚       (Artifact)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
      â”‚          â”‚
     âœ“           âœ—
     â”‚          Drift Detected
     â”‚           â”‚
     â”‚           â–¼
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    â”‚ RETRAIN MODEL    â”‚
     â”‚    â”‚ (Bayesian Opt)   â”‚
     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚
     â”‚             â–¼
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼    â”‚ Test & Validate  â”‚
  Continue â”‚   New Model      â”‚
     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Deploy to Azure  â”‚
     â”‚ Container Apps   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Seuils recommandÃ©s

### Pour donnÃ©es financiÃ¨res (BTC) :

```python
# DÃ©tection stricte (plus de retrainings)
ks_threshold = 0.01      # KS p-value trÃ¨s strict
psi_threshold = 0.05     # PSI trÃ¨s strict
ttest_threshold = 0.01   # t-test trÃ¨s strict

# DÃ©tection modÃ©rÃ©e (balance)
ks_threshold = 0.05      # KS p-value standard
psi_threshold = 0.10     # PSI modÃ©rÃ©
ttest_threshold = 0.05   # t-test standard

# DÃ©tection souple (peu de retrainings)
ks_threshold = 0.10      # KS p-value souple
psi_threshold = 0.25     # PSI souple
ttest_threshold = 0.10   # t-test souple
```

---

## ğŸš€ Commandes utiles

### Tester localement :
```bash
cd training

# Test unitaire
python test_drift_detection.py

# DÃ©tection simple
python detect_drift.py

# Simulation simple
python simulate_drift.py
```

### GÃ©nÃ©rer scÃ©narios de drift :
```bash
python simulate_drift.py
# GÃ©nÃ¨re: training/drift_scenarios/{scenario}.csv
```

### Checker les rÃ©sultats :
```bash
# Afficher le JSON des rÃ©sultats
cat drift_detection_results.json | jq .

# Ou juste les features avec drift
cat drift_detection_results.json | jq '.summary.drifted_features'
```

---

## ğŸ“Š MÃ©triques de monitoring

Pour Azure Log Analytics, on peut envoyer :

```python
{
    "timestamp": "2026-01-08T12:30:00",
    "detection_method": "psi",
    "drifted_features": ["volume_sma", "rsi_14"],
    "drift_severity": "moderate",
    "recommended_action": "Monitor closely",
    "psi_scores": {
        "volume_sma": 0.15,
        "rsi_14": 0.12,
        "price_close": 0.03
    }
}
```

---

## ğŸ” Bonnes pratiques

1. **Frequency** : VÃ©rifier le drift **hebdomadairement** minimum
2. **Thresholds** : Adapter les seuils au domaine (finance = stricte)
3. **Actions** : Avoir une procÃ©dure de rÃ©action dÃ©finie
4. **Logging** : Tracker tous les drifts dÃ©tectÃ©s
5. **Feedback** : Valider que le retraining amÃ©liore les performances

---

## âŒ PiÃ¨ges courants

```python
# âŒ MAUVAIS: Ignorer le drift
# ModÃ¨les se dÃ©gradent silencieusement

# âœ“ BON: Monitorer rÃ©guliÃ¨rement
detector = DataDriftDetector(ref_data)
results = detector.detect_drift(prod_data)

# âŒ MAUVAIS: Un seul test statistique
# Peut Ãªtre faux positif

# âœ“ BON: Combiner plusieurs mÃ©thodes
methods=['ks', 'psi', 'ttest']  # Consensus requis

# âŒ MAUVAIS: Seuils trop souples
# Manque les drifts importants

# âœ“ BON: Seuils adaptÃ©s au domaine
ks_threshold=0.05, psi_threshold=0.10
```

---

## ğŸ“ Next Steps

- [ ] IntÃ©grer alertes Slack sur drift dÃ©tectÃ©
- [ ] Ajouter mÃ©triques de performance modÃ¨le
- [ ] ImplÃ©menter rÃ©cupÃ©ration de donnÃ©es en batch
- [ ] Ajouter adaptive thresholds basÃ©s sur l'historique
- [ ] Dashboard de monitoring en temps rÃ©el
